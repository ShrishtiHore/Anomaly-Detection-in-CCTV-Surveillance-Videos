{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported successfully !!\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries and packages\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "print(\"libraries imported successfully !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset imported successfully !!\n"
     ]
    }
   ],
   "source": [
    "#Importing our dataset from ucsd anaomaly site \n",
    "tar = 'C:\\\\Users\\\\Shrishti D Hore\\\\OneDrive\\\\Documents\\\\HEU_AI\\\\UCSD_Anomaly_Dataset.v1p2'\n",
    "print(\"dataset imported successfully !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder and decoder layers set for the neural network!!!!\n"
     ]
    }
   ],
   "source": [
    "#We are using Convolutional Autoencoder(CAE) for good reason that they can be trained in normal parts and wont require annotated data.\n",
    "#Once trained we can give a featured representation for a part and compare autoencoder output and input\n",
    "#Concept is the larger the difference the more likely the anomaly\n",
    "#Two parts of the autoencoder : encoder and decoder\n",
    "#Here encoder will encode the input data using a reduced representation and the decoder will attempt to re-construct the original input data from reduced representation\n",
    "#In our CAE model encoder will consist of 2 convolutional and 2 Max-Pooling layers and decoder will consists of two Unsampling and Two Deconvolutions\n",
    "class ConvolutionalAutoencoder(gluon.nn.HybridBlock):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = gluon.nn.HybridSequential() #for stacking the hybrid blocks sequentially\n",
    "            with self.encoder.name_scope():\n",
    "                self.encoder.add(gluon.nn.Conv2D(32, 5, activation='relu')) #2D convolution layer with 32 channels and kernel_size set to 5 with activation function as Relu\n",
    "                self.encoder.add(gluon.nn.MaxPool2D(2)) #maxpooling for our 2D spatial data\n",
    "                self.encoder.add(gluon.nn.Conv2D(32, 5, activation='relu')) \n",
    "                self.encoder.add(gluon.nn.MaxPool2D(2))\n",
    "                self.encoder.add(gluon.nn.Dense(2000))\n",
    "\n",
    "            self.decoder = gluon.nn.HybridSequential()\n",
    "            with self.decoder.name_scope():\n",
    "                self.decoder.add(gluon.nn.Dense(32*22*22, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.HybridLambda(lambda F, x: F.UpSampling(x, scale=2, sample_type='nearest'))) #taking a lambda function to wrap our operations as a Hybridblock object\n",
    "                self.decoder.add(gluon.nn.Conv2DTranspose(32, 5, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.HybridLambda(lambda F, x: F.UpSampling(x, scale=2, sample_type='nearest')))\n",
    "                self.decoder.add(gluon.nn.Conv2DTranspose(1, kernel_size=5, activation='sigmoid')) #1 dense layer with kernel size to 5 and activation function as sigmoid\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder[0](x)\n",
    "        \n",
    "        #needs to be reshaped output vector from Dense(32*22*22), before it is unsampled\n",
    "        x = x.reshape((-1,32,22,22))\n",
    "        x = self.decoder[1:](x)\n",
    "\n",
    "        return x\n",
    "print(\"Encoder and decoder layers set for the neural network!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training of autoencoder with bachsize 32 set !!!\n"
     ]
    }
   ],
   "source": [
    "#Training the Autoencoder for 30 epochs and setting the batch size to 32\n",
    "#Main traing loop for the CAE using previously set epochs and batch size\n",
    "#Here we will compute loss function backwards computing dloss/dx for every parameter \n",
    "#Here the optimizer step will update the value of parameter using gradient \n",
    "\n",
    "ctx = mx.cpu()\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "# Train the autoencoder\n",
    "def train(batch_size, ctx, num_epochs, path, lr=1e-4, wd=1e-5, params_file=\"autoencoder_ucsd_convae.params\"):\n",
    "    \n",
    "    # Dataloader for training dataset\n",
    "    dataloader = utils.create_dataset(path, batch_size, shuffle=True)\n",
    "    \n",
    "    # Get model\n",
    "    model = ConvolutionalAutoencoder()\n",
    "    model.hybridize()\n",
    "    \n",
    "    # Initialiize\n",
    "    model.collect_params().initialize(mx.init.Xavier('gaussian'), ctx=ctx)\n",
    "    \n",
    "    # Loss\n",
    "    l2loss = gluon.loss.L2Loss()\n",
    "    optimizer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr, 'wd': wd})\n",
    "    \n",
    "    # Start training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        \n",
    "        for image in dataloader:\n",
    "            \n",
    "            image = image.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                \n",
    "                reconstructed = model(image)\n",
    "                loss = l2loss(reconstructed, image)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step(batch_size)\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, mx.nd.mean(loss).asscalar()))\n",
    "    \n",
    "    # Save parameters\n",
    "    model.save_parameters(params_file)\n",
    "    return model, params_file\n",
    "\n",
    "print(\"training of autoencoder with bachsize 32 set !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaled and normalized images successfully !!!\n"
     ]
    }
   ],
   "source": [
    "#SInce the images in the UCSDpeds1 folder in UCSD_Anomaly_Dataset.v1p2 main folder have the image format of 158x238 pixels\n",
    "#We need to rescale the images to 100x100 and normalize them\n",
    "files = sorted(glob.glob('UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/*/*'))\n",
    "\n",
    "a = np.zeros((len(files),1,100,100))\n",
    "\n",
    "for idx, filename in enumerate(files):\n",
    "    im = Image.open('C:\\\\Users\\\\Shrishti D Hore\\\\OneDrive\\\\Documents\\\\HEU_AI\\\\UCSD_Anomaly_Dataset.v1p2\\\\UCSDped1\\\\Train\\\\Train001')\n",
    "    im = im.resize((100,100))\n",
    "    a[idx,0,:,:] = np.array(im, dtype=np.float32)/255.0\n",
    "\n",
    "dataset = gluon.data.ArrayDataset(mx.nd.array(a, dtype=np.float32))\n",
    "dataloader = gluon.data.DataLoader(dataset, batch_size=batch_size, last_batch='rollover',shuffle=True)\n",
    "print(\"Rescaled and normalized images successfully !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined loss function successfully !!!\n"
     ]
    }
   ],
   "source": [
    "#Now we need to initialize and define our loss function of the Convolutional Autoencoder\n",
    "model = ConvolutionalAutoencoder()\n",
    "model.hybridize()\n",
    "model.collect_params().initialize(mx.init.Xavier('gaussian'), ctx=ctx)\n",
    "loss_function = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 1e-4, 'wd': 1e-5})\n",
    "print(\"defined loss function successfully !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
